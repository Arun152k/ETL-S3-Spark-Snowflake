24/06/13 23:52:37 WARN Utils: Your hostname, Arun resolves to a loopback address: 127.0.1.1; using 172.17.155.246 instead (on interface eth0)
24/06/13 23:52:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/mnt/c/Users/rarun/Desktop/DataEng/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
:: loading settings :: url = jar:file:/mnt/c/Users/rarun/Desktop/DataEng/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/arun/.ivy2/cache
The jars for the packages stored in: /home/arun/.ivy2/jars
net.snowflake#snowflake-jdbc added as a dependency
net.snowflake#spark-snowflake_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-da7c44b4-6e93-40de-bbae-8f49f1a7bf81;1.0
	confs: [default]
	found net.snowflake#snowflake-jdbc;3.12.0 in central
	found net.snowflake#spark-snowflake_2.12;2.9.0-spark_3.1 in central
	found net.snowflake#snowflake-ingest-sdk;0.10.2 in central
	found net.snowflake#snowflake-jdbc;3.13.3 in central
:: resolution report :: resolve 501ms :: artifacts dl 11ms
	:: modules in use:
	net.snowflake#snowflake-ingest-sdk;0.10.2 from central in [default]
	net.snowflake#snowflake-jdbc;3.13.3 from central in [default]
	net.snowflake#spark-snowflake_2.12;2.9.0-spark_3.1 from central in [default]
	:: evicted modules:
	net.snowflake#snowflake-jdbc;3.12.0 by [net.snowflake#snowflake-jdbc;3.13.3] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-da7c44b4-6e93-40de-bbae-8f49f1a7bf81
	confs: [default]
	0 artifacts copied, 3 already retrieved (0kB/27ms)
24/06/13 23:52:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
INFO:__main__:Spark session created successfully.
INFO:__main__:AWS client created successfully
INFO:__main__:2 files present in the S3 bucket

[Stage 0:>                                                          (0 + 1) / 1]

                                                                                

[Stage 1:>                                                          (0 + 4) / 4]

                                                                                

[Stage 2:>                                                        (0 + 11) / 11]

[Stage 2:====================================>                     (7 + 4) / 11]

                                                                                
INFO:__main__:name and location columns have nested values

[Stage 3:>                                                        (0 + 16) / 16]

                                                                                

[Stage 4:>                                                        (0 + 16) / 16]

[Stage 4:=======================================>                 (11 + 5) / 16]

                                                                                

[Stage 5:>                                                          (0 + 1) / 1]

                                                                                

[Stage 6:>                                                          (0 + 4) / 4]

                                                                                

[Stage 7:>                                                        (0 + 11) / 11]

                                                                                

[Stage 8:>                                                        (0 + 16) / 16]

                                                                                
INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.10.1, Python Version: 3.10.12, Platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
INFO:__main__:Connection successful!
INFO:snowflake.connector.cursor:Number of results in first chunk: 1
INFO:__main__:Schema 's3Sink' created successfully
INFO:snowflake.connector.cursor:Number of results in first chunk: 1
INFO:snowflake.connector.connection:closed
INFO:snowflake.connector.connection:No async queries seem to be running, deleting session
INFO:__main__:Connection closed

[Stage 9:>                                                        (0 + 16) / 16]

[Stage 9:=================================================>       (14 + 2) / 16]

                                                                                
INFO:__main__:DataFrame succesfully inserted to Snowflake
Files in bucket:
  1. user1.json
  2. user2.json
Full DataFrame
+----------+--------------------+------+--------------------+--------------------+--------------+----------+
|       dob|               email|gender|            location|                name|         phone|registered|
+----------+--------------------+------+--------------------+--------------------+--------------+----------+
|1960-09-07|shahistha.nair@ex...|female|{postcode -> 4501...|{last -> Nair, fi...|    8399279924|2006-07-06|
|1987-05-19|fatih.onur@exampl...|  male|{postcode -> 1043...|{last -> Önür, fi...|(003)-464-2725|2003-09-24|
+----------+--------------------+------+--------------------+--------------------+--------------+----------+


Printing the first row of the longer columns (name, location)
name column Row(name={'last': 'Nair', 'first': 'Shahistha'})
location column Row(location={'postcode': '45018', 'country': 'India', 'state': 'Haryana', 'city': 'Guwahati', 'street': '1871 Carter Rd Promenade'})
Full DataFrame
+----------+--------------------+------+--------------------+----------+----------+---------+--------+
|       dob|               email|gender|            location|     phone|registered|firstName|lastName|
+----------+--------------------+------+--------------------+----------+----------+---------+--------+
|1960-09-07|shahistha.nair@ex...|female|1871 Carter Rd Pr...|8399279924|2006-07-06|Shahistha|    Nair|
|1987-05-19|fatih.onur@exampl...|  male|1625 Tunalı Hilmi...|0034642725|2003-09-24|    Fatih|    Önür|
+----------+--------------------+------+--------------------+----------+----------+---------+--------+


Printing the first row of the firstName, lastName, location, phone
Row(firstName='Shahistha', lastName='Nair', location='1871 Carter Rd Promenade, Guwahati, Haryana, 45018, India', phone='8399279924')
Table 'users' created successfully in schema 's3Sink'